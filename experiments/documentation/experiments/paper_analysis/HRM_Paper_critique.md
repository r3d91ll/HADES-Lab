# The hierarchical reasoning model critique reveals memorization over innovation

***Hierarchical Reasoning Model Paper<https://arxiv.org/html/2506.21734v3>***

The Hierarchical Reasoning Model (HRM), released in June 2025 by Sapient Intelligence, claims to achieve breakthrough performance on reasoning tasks through brain-inspired architecture, but independent verification reveals its success stems primarily from test-time training and data augmentation rather than architectural innovation. The ARC Prize organization's comprehensive analysis found that a standard transformer with matched parameters achieves within 5 percentage points of HRM's performance without any hyperparameter optimization, severely undermining the paper's central claims about hierarchical reasoning. Most critically, the analysis revealed that 75% of HRM's performance derives from memorizing solutions to evaluation tasks, making it fundamentally similar to existing "ARC-AGI without pretraining" approaches rather than representing a novel reasoning paradigm. The model's impressive 41% score on public ARC-AGI-1 tasks drops to 32% on hidden test sets and plummets to just 2% on the harder ARC-AGI-2 benchmark, indicating severe limitations in generalization beyond memorized patterns.

## Academic critique exposes limited peer review and overstated claims

The academic community's response to HRM has been notably limited, with no formal peer reviews from major ML conferences and minimal engagement from prominent researchers like Yann LeCun or Geoffrey Hinton, despite the paper's viral reception garnering over 4 million views on social media. The most substantive critique comes from the ARC Prize organization's independent verification led by researcher Konstantin Sch√ºrholt, which systematically dismantled the paper's core claims through rigorous ablation studies. Their analysis revealed that HRM's purported advantages stem not from its hierarchical architecture but from an under-documented "outer loop" refinement process that drives +13 percentage points of performance improvement. Technical discussions on platforms like Hacker News raised concerns about potential overfitting, with one commenter noting "this smells like some kind of overfit," while the System D blog identified "classic survivorship bias" in the paper's selective benchmark reporting.

The paper's claim of using only "1000 training samples" proves misleading, as the actual training process involves 300-1000 augmentations per task, creating effectively millions of training examples. Independent reproduction attempts consistently achieve lower performance than claimed, with HigherOrderCO reporting only 45.8% accuracy on Sudoku-Extreme versus the claimed 55%, and approximately 25% on ARC-AGI versus the claimed 40.3%, suggesting either undisclosed training details or cherry-picked results in the original paper.

## Technical mechanisms fail theoretical scrutiny

HRM's hierarchical convergence mechanism, inspired by brain oscillatory patterns with separate high-level (H) and low-level (L) modules, shows minimal impact on performance when subjected to rigorous testing. The one-step gradient approximation method, based on Deep Equilibrium Models theory, lacks formal guarantees for fixed point existence and uniqueness, raising serious stability concerns that recent research on "Positive Concave Deep Equilibrium Models" highlights as "potentially unstable in practice." The paper's brain-inspired principles, while generating interesting emergent properties like dimensional separation between modules (H-module PR=89.95 vs L-module PR=30.22), provide only correlational evidence without demonstrating causal necessity.

**The coupling between H and L modules uses simple element-wise addition** rather than sophisticated gating mechanisms, and both modules employ identical transformer architectures, undermining claims of fundamental architectural innovation. Technical analysis reveals HRM's approach closely resembles existing methods like Universal Transformers with adaptive computation time, rather than representing a breakthrough in hierarchical reasoning.

## Implementation challenges reveal fundamental limitations

Multiple independent implementation attempts exposed critical limitations not adequately disclosed in the original paper. The model requires puzzle-specific ID embeddings that prevent generalization to truly novel tasks, meaning it "can only be applied on puzzles with puzzle_ids it has seen at training time." This dependency creates what researchers call "complex engineering challenges" for real-world applications and severely limits the model's ability to handle few-shot contexts or transfer learning scenarios.

GitHub repositories show numerous forks but few meaningful independent implementations, with most being direct copies of the official code. Technical issues include PyTorch compatibility problems, training instability from late-stage overfitting, incomplete requirements specifications, and complex CUDA extension dependencies that make reproduction difficult for researchers without specialized hardware access.

## Performance comparisons demolish architectural superiority claims

Direct comparisons with other reasoning architectures reveal HRM's claimed advantages disappear under controlled conditions. **A standard transformer achieves within 5 percentage points of HRM's performance**, suggesting the hierarchical architecture provides minimal benefit beyond what refinement loops and data augmentation achieve. The model excels only on narrow algorithmic puzzles like custom Sudoku-Extreme and Maze-Hard variants where competing models score 0%, but these are specifically designed benchmarks rather than standard evaluation sets.

When tested on hidden datasets, HRM's performance degrades significantly: from 41% on public ARC-AGI-1 to 32% on semi-private sets, and achieving only 2% on ARC-AGI-2, which the ARC Prize organization considers "not meaningful progress." Analysis shows that training on only the 400 evaluation tasks (excluding training and ConceptARC data) still achieves 31% accuracy, confirming that memorization rather than reasoning drives performance.

## Critical limitations emerge from test-time training dependency

The most damning finding reveals that HRM functions essentially as a test-time training system rather than a generalizable reasoning model, with performance heavily dependent on training directly on evaluation tasks. **Approximately 75% of performance comes from memorizing solutions** to specific evaluation tasks through extensive augmentation, making claims about "1000 training samples" highly misleading when actual training involves millions of augmented examples. The model cannot process tasks with unseen puzzle IDs, severely limiting real-world applicability, and shows no evidence of genuine cross-task transfer learning or compositional generalization.

Inference-time augmentation and majority voting add minimal value compared to training-time augmentation, suggesting the model's strength lies in preprocessing rather than dynamic reasoning. The purely transductive nature means the underlying program remains implicit and unlikely to generalize beyond memorized patterns, raising fundamental questions about whether HRM represents progress toward artificial general intelligence or merely sophisticated pattern matching.

## Conclusion

While HRM demonstrates technical competence in implementation and achieves impressive results on specific benchmarks, independent analysis reveals its core innovations are significantly overstated. The model's success stems primarily from effective data augmentation and test-time training rather than novel architectural breakthroughs in hierarchical reasoning. The limited formal academic critique, consistent underperformance in reproductions, and fundamental architectural limitations exposed by the ARC Prize analysis suggest HRM represents an incremental refinement of existing techniques rather than the paradigm shift its marketing suggests. Future work should focus on genuine architectural innovations that enable compositional generalization rather than sophisticated memorization, and the ML community should demand more rigorous evaluation protocols that prevent selective reporting and overstated claims.