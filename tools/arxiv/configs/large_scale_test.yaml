# Large-Scale ArXiv Processing Test Configuration
# ================================================
# Optimized for processing 5000+ papers

pipeline:
  name: "large_scale_arxiv_test"
  version: "1.0.0"
  description: "Test configuration for 5000+ AI/RAG/LLM/ANT papers"

# Phase-separated processing
phases:
  extraction:
    workers: 32  # CPU workers for extraction
    batch_size: 32
    timeout_seconds: 60
    use_ocr: false  # Disable OCR for speed
    extract_tables: true
    extract_equations: true
    
  embedding:
    workers: 8  # GPU workers (4 per GPU)
    batch_size: 24
    gpu_devices: [0, 1]  # Use both GPUs
    use_fp16: true  # Half precision for memory efficiency
    late_chunking: true
    max_context_tokens: 32000

# Staging configuration
staging:
  directory: "/dev/shm/large_scale_staging"
  cleanup_on_success: true
  cleanup_on_failure: false  # Keep for debugging

# Memory optimization
memory:
  gpu_memory_fraction: 0.8  # Use 80% of GPU memory
  clear_gpu_between_phases: true
  worker_restart_interval: 100  # Restart workers every 100 papers

# Processing configuration
processing:
  pdf_base_dir: "/bulk-store/arxiv-data/pdf"
  max_file_size_mb: 50
  skip_existing: true  # Skip already processed papers
  checkpoint_interval: 100  # Save checkpoint every 100 papers
  retry_failed: true
  max_retries: 3

# ArangoDB configuration
arango:
  host: "http://192.168.1.69:8529"
  database: "academy_store"
  username: "root"
  # password will be provided at runtime
  
  # Collection names
  collections:
    papers: "arxiv_papers"
    chunks: "arxiv_chunks"
    embeddings: "arxiv_embeddings"
    structures: "arxiv_structures"
  
  # Batch settings
  batch_insert_size: 100
  use_transactions: true
  transaction_size: 50

# Error handling
error_handling:
  continue_on_error: true
  log_errors: true
  error_log_file: "logs/large_scale_errors.log"
  save_failed_list: true
  failed_list_file: "logs/failed_papers.txt"

# Performance monitoring
monitoring:
  enable_metrics: true
  metrics_interval: 100  # Report every 100 papers
  track_gpu_usage: true
  track_memory_usage: true
  log_level: "INFO"

# Rate limiting (for API calls if needed)
rate_limiting:
  enable: false  # We're processing local files
  requests_per_second: 10
  burst_size: 20

# Parallel processing strategy
parallel:
  extraction_strategy: "process_pool"  # Use ProcessPoolExecutor
  embedding_strategy: "gpu_parallel"   # GPU-based parallelism
  storage_strategy: "batch"            # Batch database writes

# Optimization flags
optimizations:
  use_ramfs: true              # Use RAM filesystem for staging
  prefetch_pdfs: true          # Prefetch next batch while processing
  compress_staging: false      # Don't compress (we have RAM)
  cache_embeddings: false      # Don't cache (too many papers)
  enable_profiling: false      # Disable for production run

# Test-specific settings
test:
  sample_rate: 1.0            # Process all papers (no sampling)
  validation_checks: true     # Run validation after processing
  generate_report: true       # Generate test report
  report_format: "json"       # Report format
  compare_baselines: true     # Compare against baseline metrics